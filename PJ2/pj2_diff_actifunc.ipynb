{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f604ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e16103f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a26c43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0335600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# preprocess\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "497f08e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GELU\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes * self.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes * self.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * self.expansion)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.gelu(self.bn1(self.conv1(x)))\n",
    "        out = F.gelu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.gelu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2494568d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Half filters\n",
    "# GELU\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 32\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.layer1 = self._make_layer(block, 32, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 64, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 128, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 256, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(256 * block.expansion, num_classes)\n",
    "    \n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.gelu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, out.size()[2:])\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "def Net():\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca57b9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "202d3bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(num_epochs=100):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, (images, labels) in enumerate(trainloader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}], Loss: {running_loss/100:.4f}')\n",
    "                running_loss = 0.0\n",
    "        scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aed7eded",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f'Accuracy on test set: {100 * correct / total:.2f}%')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dba5465f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Step [100], Loss: 2.0737\n",
      "Epoch [1/100], Step [200], Loss: 1.7491\n",
      "Epoch [1/100], Step [300], Loss: 1.5644\n",
      "Epoch [2/100], Step [100], Loss: 1.2543\n",
      "Epoch [2/100], Step [200], Loss: 1.1613\n",
      "Epoch [2/100], Step [300], Loss: 1.0847\n",
      "Epoch [3/100], Step [100], Loss: 0.9321\n",
      "Epoch [3/100], Step [200], Loss: 0.9054\n",
      "Epoch [3/100], Step [300], Loss: 0.8697\n",
      "Epoch [4/100], Step [100], Loss: 0.7927\n",
      "Epoch [4/100], Step [200], Loss: 0.7568\n",
      "Epoch [4/100], Step [300], Loss: 0.7325\n",
      "Epoch [5/100], Step [100], Loss: 0.6692\n",
      "Epoch [5/100], Step [200], Loss: 0.6491\n",
      "Epoch [5/100], Step [300], Loss: 0.6439\n",
      "Epoch [6/100], Step [100], Loss: 0.5916\n",
      "Epoch [6/100], Step [200], Loss: 0.5783\n",
      "Epoch [6/100], Step [300], Loss: 0.5753\n",
      "Epoch [7/100], Step [100], Loss: 0.5391\n",
      "Epoch [7/100], Step [200], Loss: 0.5324\n",
      "Epoch [7/100], Step [300], Loss: 0.5341\n",
      "Epoch [8/100], Step [100], Loss: 0.5105\n",
      "Epoch [8/100], Step [200], Loss: 0.5019\n",
      "Epoch [8/100], Step [300], Loss: 0.4939\n",
      "Epoch [9/100], Step [100], Loss: 0.4633\n",
      "Epoch [9/100], Step [200], Loss: 0.4665\n",
      "Epoch [9/100], Step [300], Loss: 0.4651\n",
      "Epoch [10/100], Step [100], Loss: 0.4435\n",
      "Epoch [10/100], Step [200], Loss: 0.4420\n",
      "Epoch [10/100], Step [300], Loss: 0.4352\n",
      "Epoch [11/100], Step [100], Loss: 0.4137\n",
      "Epoch [11/100], Step [200], Loss: 0.4173\n",
      "Epoch [11/100], Step [300], Loss: 0.4224\n",
      "Epoch [12/100], Step [100], Loss: 0.3916\n",
      "Epoch [12/100], Step [200], Loss: 0.3839\n",
      "Epoch [12/100], Step [300], Loss: 0.3914\n",
      "Epoch [13/100], Step [100], Loss: 0.3529\n",
      "Epoch [13/100], Step [200], Loss: 0.3701\n",
      "Epoch [13/100], Step [300], Loss: 0.3977\n",
      "Epoch [14/100], Step [100], Loss: 0.3470\n",
      "Epoch [14/100], Step [200], Loss: 0.3599\n",
      "Epoch [14/100], Step [300], Loss: 0.3711\n",
      "Epoch [15/100], Step [100], Loss: 0.3308\n",
      "Epoch [15/100], Step [200], Loss: 0.3513\n",
      "Epoch [15/100], Step [300], Loss: 0.3470\n",
      "Epoch [16/100], Step [100], Loss: 0.3212\n",
      "Epoch [16/100], Step [200], Loss: 0.3298\n",
      "Epoch [16/100], Step [300], Loss: 0.3345\n",
      "Epoch [17/100], Step [100], Loss: 0.3109\n",
      "Epoch [17/100], Step [200], Loss: 0.3170\n",
      "Epoch [17/100], Step [300], Loss: 0.3236\n",
      "Epoch [18/100], Step [100], Loss: 0.2943\n",
      "Epoch [18/100], Step [200], Loss: 0.3023\n",
      "Epoch [18/100], Step [300], Loss: 0.3077\n",
      "Epoch [19/100], Step [100], Loss: 0.2826\n",
      "Epoch [19/100], Step [200], Loss: 0.2817\n",
      "Epoch [19/100], Step [300], Loss: 0.3007\n",
      "Epoch [20/100], Step [100], Loss: 0.2646\n",
      "Epoch [20/100], Step [200], Loss: 0.2951\n",
      "Epoch [20/100], Step [300], Loss: 0.2997\n",
      "Epoch [21/100], Step [100], Loss: 0.2632\n",
      "Epoch [21/100], Step [200], Loss: 0.2624\n",
      "Epoch [21/100], Step [300], Loss: 0.2900\n",
      "Epoch [22/100], Step [100], Loss: 0.2564\n",
      "Epoch [22/100], Step [200], Loss: 0.2661\n",
      "Epoch [22/100], Step [300], Loss: 0.2772\n",
      "Epoch [23/100], Step [100], Loss: 0.2431\n",
      "Epoch [23/100], Step [200], Loss: 0.2463\n",
      "Epoch [23/100], Step [300], Loss: 0.2601\n",
      "Epoch [24/100], Step [100], Loss: 0.2368\n",
      "Epoch [24/100], Step [200], Loss: 0.2528\n",
      "Epoch [24/100], Step [300], Loss: 0.2438\n",
      "Epoch [25/100], Step [100], Loss: 0.2224\n",
      "Epoch [25/100], Step [200], Loss: 0.2371\n",
      "Epoch [25/100], Step [300], Loss: 0.2373\n",
      "Epoch [26/100], Step [100], Loss: 0.2137\n",
      "Epoch [26/100], Step [200], Loss: 0.2207\n",
      "Epoch [26/100], Step [300], Loss: 0.2360\n",
      "Epoch [27/100], Step [100], Loss: 0.2056\n",
      "Epoch [27/100], Step [200], Loss: 0.2200\n",
      "Epoch [27/100], Step [300], Loss: 0.2264\n",
      "Epoch [28/100], Step [100], Loss: 0.1977\n",
      "Epoch [28/100], Step [200], Loss: 0.2084\n",
      "Epoch [28/100], Step [300], Loss: 0.2207\n",
      "Epoch [29/100], Step [100], Loss: 0.1935\n",
      "Epoch [29/100], Step [200], Loss: 0.2093\n",
      "Epoch [29/100], Step [300], Loss: 0.2105\n",
      "Epoch [30/100], Step [100], Loss: 0.1818\n",
      "Epoch [30/100], Step [200], Loss: 0.1976\n",
      "Epoch [30/100], Step [300], Loss: 0.2128\n",
      "Epoch [31/100], Step [100], Loss: 0.1875\n",
      "Epoch [31/100], Step [200], Loss: 0.1946\n",
      "Epoch [31/100], Step [300], Loss: 0.1920\n",
      "Epoch [32/100], Step [100], Loss: 0.1720\n",
      "Epoch [32/100], Step [200], Loss: 0.1909\n",
      "Epoch [32/100], Step [300], Loss: 0.1915\n",
      "Epoch [33/100], Step [100], Loss: 0.1696\n",
      "Epoch [33/100], Step [200], Loss: 0.1858\n",
      "Epoch [33/100], Step [300], Loss: 0.1924\n",
      "Epoch [34/100], Step [100], Loss: 0.1600\n",
      "Epoch [34/100], Step [200], Loss: 0.1701\n",
      "Epoch [34/100], Step [300], Loss: 0.1808\n",
      "Epoch [35/100], Step [100], Loss: 0.1578\n",
      "Epoch [35/100], Step [200], Loss: 0.1618\n",
      "Epoch [35/100], Step [300], Loss: 0.1618\n",
      "Epoch [36/100], Step [100], Loss: 0.1657\n",
      "Epoch [36/100], Step [200], Loss: 0.1474\n",
      "Epoch [36/100], Step [300], Loss: 0.1735\n",
      "Epoch [37/100], Step [100], Loss: 0.1519\n",
      "Epoch [37/100], Step [200], Loss: 0.1539\n",
      "Epoch [37/100], Step [300], Loss: 0.1726\n",
      "Epoch [38/100], Step [100], Loss: 0.1380\n",
      "Epoch [38/100], Step [200], Loss: 0.1464\n",
      "Epoch [38/100], Step [300], Loss: 0.1571\n",
      "Epoch [39/100], Step [100], Loss: 0.1434\n",
      "Epoch [39/100], Step [200], Loss: 0.1521\n",
      "Epoch [39/100], Step [300], Loss: 0.1451\n",
      "Epoch [40/100], Step [100], Loss: 0.1361\n",
      "Epoch [40/100], Step [200], Loss: 0.1371\n",
      "Epoch [40/100], Step [300], Loss: 0.1508\n",
      "Epoch [41/100], Step [100], Loss: 0.1284\n",
      "Epoch [41/100], Step [200], Loss: 0.1330\n",
      "Epoch [41/100], Step [300], Loss: 0.1375\n",
      "Epoch [42/100], Step [100], Loss: 0.1238\n",
      "Epoch [42/100], Step [200], Loss: 0.1309\n",
      "Epoch [42/100], Step [300], Loss: 0.1324\n",
      "Epoch [43/100], Step [100], Loss: 0.1255\n",
      "Epoch [43/100], Step [200], Loss: 0.1198\n",
      "Epoch [43/100], Step [300], Loss: 0.1310\n",
      "Epoch [44/100], Step [100], Loss: 0.1160\n",
      "Epoch [44/100], Step [200], Loss: 0.1146\n",
      "Epoch [44/100], Step [300], Loss: 0.1158\n",
      "Epoch [45/100], Step [100], Loss: 0.1124\n",
      "Epoch [45/100], Step [200], Loss: 0.1172\n",
      "Epoch [45/100], Step [300], Loss: 0.1229\n",
      "Epoch [46/100], Step [100], Loss: 0.1114\n",
      "Epoch [46/100], Step [200], Loss: 0.1049\n",
      "Epoch [46/100], Step [300], Loss: 0.1083\n",
      "Epoch [47/100], Step [100], Loss: 0.0935\n",
      "Epoch [47/100], Step [200], Loss: 0.1078\n",
      "Epoch [47/100], Step [300], Loss: 0.1131\n",
      "Epoch [48/100], Step [100], Loss: 0.1035\n",
      "Epoch [48/100], Step [200], Loss: 0.1080\n",
      "Epoch [48/100], Step [300], Loss: 0.1063\n",
      "Epoch [49/100], Step [100], Loss: 0.0895\n",
      "Epoch [49/100], Step [200], Loss: 0.0883\n",
      "Epoch [49/100], Step [300], Loss: 0.1008\n",
      "Epoch [50/100], Step [100], Loss: 0.0835\n",
      "Epoch [50/100], Step [200], Loss: 0.0915\n",
      "Epoch [50/100], Step [300], Loss: 0.0982\n",
      "Epoch [51/100], Step [100], Loss: 0.0891\n",
      "Epoch [51/100], Step [200], Loss: 0.0862\n",
      "Epoch [51/100], Step [300], Loss: 0.0911\n",
      "Epoch [52/100], Step [100], Loss: 0.0814\n",
      "Epoch [52/100], Step [200], Loss: 0.0854\n",
      "Epoch [52/100], Step [300], Loss: 0.0875\n",
      "Epoch [53/100], Step [100], Loss: 0.0700\n",
      "Epoch [53/100], Step [200], Loss: 0.0874\n",
      "Epoch [53/100], Step [300], Loss: 0.0880\n",
      "Epoch [54/100], Step [100], Loss: 0.0674\n",
      "Epoch [54/100], Step [200], Loss: 0.0708\n",
      "Epoch [54/100], Step [300], Loss: 0.0807\n",
      "Epoch [55/100], Step [100], Loss: 0.0708\n",
      "Epoch [55/100], Step [200], Loss: 0.0798\n",
      "Epoch [55/100], Step [300], Loss: 0.0732\n",
      "Epoch [56/100], Step [100], Loss: 0.0668\n",
      "Epoch [56/100], Step [200], Loss: 0.0629\n",
      "Epoch [56/100], Step [300], Loss: 0.0720\n",
      "Epoch [57/100], Step [100], Loss: 0.0607\n",
      "Epoch [57/100], Step [200], Loss: 0.0597\n",
      "Epoch [57/100], Step [300], Loss: 0.0735\n",
      "Epoch [58/100], Step [100], Loss: 0.0624\n",
      "Epoch [58/100], Step [200], Loss: 0.0646\n",
      "Epoch [58/100], Step [300], Loss: 0.0638\n",
      "Epoch [59/100], Step [100], Loss: 0.0583\n",
      "Epoch [59/100], Step [200], Loss: 0.0578\n",
      "Epoch [59/100], Step [300], Loss: 0.0627\n",
      "Epoch [60/100], Step [100], Loss: 0.0562\n",
      "Epoch [60/100], Step [200], Loss: 0.0493\n",
      "Epoch [60/100], Step [300], Loss: 0.0629\n",
      "Epoch [61/100], Step [100], Loss: 0.0530\n",
      "Epoch [61/100], Step [200], Loss: 0.0552\n",
      "Epoch [61/100], Step [300], Loss: 0.0505\n",
      "Epoch [62/100], Step [100], Loss: 0.0473\n",
      "Epoch [62/100], Step [200], Loss: 0.0482\n",
      "Epoch [62/100], Step [300], Loss: 0.0489\n",
      "Epoch [63/100], Step [100], Loss: 0.0438\n",
      "Epoch [63/100], Step [200], Loss: 0.0437\n",
      "Epoch [63/100], Step [300], Loss: 0.0508\n",
      "Epoch [64/100], Step [100], Loss: 0.0384\n",
      "Epoch [64/100], Step [200], Loss: 0.0476\n",
      "Epoch [64/100], Step [300], Loss: 0.0437\n",
      "Epoch [65/100], Step [100], Loss: 0.0381\n",
      "Epoch [65/100], Step [200], Loss: 0.0418\n",
      "Epoch [65/100], Step [300], Loss: 0.0421\n",
      "Epoch [66/100], Step [100], Loss: 0.0358\n",
      "Epoch [66/100], Step [200], Loss: 0.0373\n",
      "Epoch [66/100], Step [300], Loss: 0.0405\n",
      "Epoch [67/100], Step [100], Loss: 0.0336\n",
      "Epoch [67/100], Step [200], Loss: 0.0385\n",
      "Epoch [67/100], Step [300], Loss: 0.0415\n",
      "Epoch [68/100], Step [100], Loss: 0.0302\n",
      "Epoch [68/100], Step [200], Loss: 0.0352\n",
      "Epoch [68/100], Step [300], Loss: 0.0303\n",
      "Epoch [69/100], Step [100], Loss: 0.0281\n",
      "Epoch [69/100], Step [200], Loss: 0.0268\n",
      "Epoch [69/100], Step [300], Loss: 0.0332\n",
      "Epoch [70/100], Step [100], Loss: 0.0265\n",
      "Epoch [70/100], Step [200], Loss: 0.0321\n",
      "Epoch [70/100], Step [300], Loss: 0.0265\n",
      "Epoch [71/100], Step [100], Loss: 0.0263\n",
      "Epoch [71/100], Step [200], Loss: 0.0256\n",
      "Epoch [71/100], Step [300], Loss: 0.0270\n",
      "Epoch [72/100], Step [100], Loss: 0.0263\n",
      "Epoch [72/100], Step [200], Loss: 0.0222\n",
      "Epoch [72/100], Step [300], Loss: 0.0276\n",
      "Epoch [73/100], Step [100], Loss: 0.0243\n",
      "Epoch [73/100], Step [200], Loss: 0.0233\n",
      "Epoch [73/100], Step [300], Loss: 0.0208\n",
      "Epoch [74/100], Step [100], Loss: 0.0225\n",
      "Epoch [74/100], Step [200], Loss: 0.0231\n",
      "Epoch [74/100], Step [300], Loss: 0.0202\n",
      "Epoch [75/100], Step [100], Loss: 0.0201\n",
      "Epoch [75/100], Step [200], Loss: 0.0190\n",
      "Epoch [75/100], Step [300], Loss: 0.0216\n",
      "Epoch [76/100], Step [100], Loss: 0.0174\n",
      "Epoch [76/100], Step [200], Loss: 0.0178\n",
      "Epoch [76/100], Step [300], Loss: 0.0177\n",
      "Epoch [77/100], Step [100], Loss: 0.0145\n",
      "Epoch [77/100], Step [200], Loss: 0.0154\n",
      "Epoch [77/100], Step [300], Loss: 0.0197\n",
      "Epoch [78/100], Step [100], Loss: 0.0146\n",
      "Epoch [78/100], Step [200], Loss: 0.0137\n",
      "Epoch [78/100], Step [300], Loss: 0.0137\n",
      "Epoch [79/100], Step [100], Loss: 0.0163\n",
      "Epoch [79/100], Step [200], Loss: 0.0124\n",
      "Epoch [79/100], Step [300], Loss: 0.0149\n",
      "Epoch [80/100], Step [100], Loss: 0.0131\n",
      "Epoch [80/100], Step [200], Loss: 0.0152\n",
      "Epoch [80/100], Step [300], Loss: 0.0125\n",
      "Epoch [81/100], Step [100], Loss: 0.0112\n",
      "Epoch [81/100], Step [200], Loss: 0.0134\n",
      "Epoch [81/100], Step [300], Loss: 0.0116\n",
      "Epoch [82/100], Step [100], Loss: 0.0132\n",
      "Epoch [82/100], Step [200], Loss: 0.0112\n",
      "Epoch [82/100], Step [300], Loss: 0.0109\n",
      "Epoch [83/100], Step [100], Loss: 0.0115\n",
      "Epoch [83/100], Step [200], Loss: 0.0120\n",
      "Epoch [83/100], Step [300], Loss: 0.0103\n",
      "Epoch [84/100], Step [100], Loss: 0.0090\n",
      "Epoch [84/100], Step [200], Loss: 0.0102\n",
      "Epoch [84/100], Step [300], Loss: 0.0105\n",
      "Epoch [85/100], Step [100], Loss: 0.0094\n",
      "Epoch [85/100], Step [200], Loss: 0.0082\n",
      "Epoch [85/100], Step [300], Loss: 0.0102\n",
      "Epoch [86/100], Step [100], Loss: 0.0082\n",
      "Epoch [86/100], Step [200], Loss: 0.0082\n",
      "Epoch [86/100], Step [300], Loss: 0.0078\n",
      "Epoch [87/100], Step [100], Loss: 0.0069\n",
      "Epoch [87/100], Step [200], Loss: 0.0073\n",
      "Epoch [87/100], Step [300], Loss: 0.0080\n",
      "Epoch [88/100], Step [100], Loss: 0.0072\n",
      "Epoch [88/100], Step [200], Loss: 0.0076\n",
      "Epoch [88/100], Step [300], Loss: 0.0074\n",
      "Epoch [89/100], Step [100], Loss: 0.0064\n",
      "Epoch [89/100], Step [200], Loss: 0.0078\n",
      "Epoch [89/100], Step [300], Loss: 0.0064\n",
      "Epoch [90/100], Step [100], Loss: 0.0052\n",
      "Epoch [90/100], Step [200], Loss: 0.0055\n",
      "Epoch [90/100], Step [300], Loss: 0.0058\n",
      "Epoch [91/100], Step [100], Loss: 0.0071\n",
      "Epoch [91/100], Step [200], Loss: 0.0063\n",
      "Epoch [91/100], Step [300], Loss: 0.0064\n",
      "Epoch [92/100], Step [100], Loss: 0.0060\n",
      "Epoch [92/100], Step [200], Loss: 0.0053\n",
      "Epoch [92/100], Step [300], Loss: 0.0040\n",
      "Epoch [93/100], Step [100], Loss: 0.0055\n",
      "Epoch [93/100], Step [200], Loss: 0.0050\n",
      "Epoch [93/100], Step [300], Loss: 0.0044\n",
      "Epoch [94/100], Step [100], Loss: 0.0048\n",
      "Epoch [94/100], Step [200], Loss: 0.0051\n",
      "Epoch [94/100], Step [300], Loss: 0.0057\n",
      "Epoch [95/100], Step [100], Loss: 0.0049\n",
      "Epoch [95/100], Step [200], Loss: 0.0043\n",
      "Epoch [95/100], Step [300], Loss: 0.0055\n",
      "Epoch [96/100], Step [100], Loss: 0.0038\n",
      "Epoch [96/100], Step [200], Loss: 0.0041\n",
      "Epoch [96/100], Step [300], Loss: 0.0041\n",
      "Epoch [97/100], Step [100], Loss: 0.0039\n",
      "Epoch [97/100], Step [200], Loss: 0.0045\n",
      "Epoch [97/100], Step [300], Loss: 0.0038\n",
      "Epoch [98/100], Step [100], Loss: 0.0049\n",
      "Epoch [98/100], Step [200], Loss: 0.0054\n",
      "Epoch [98/100], Step [300], Loss: 0.0052\n",
      "Epoch [99/100], Step [100], Loss: 0.0042\n",
      "Epoch [99/100], Step [200], Loss: 0.0035\n",
      "Epoch [99/100], Step [300], Loss: 0.0052\n",
      "Epoch [100/100], Step [100], Loss: 0.0042\n",
      "Epoch [100/100], Step [200], Loss: 0.0044\n",
      "Epoch [100/100], Step [300], Loss: 0.0040\n"
     ]
    }
   ],
   "source": [
    "train_model(num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "935ce16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 93.58%\n"
     ]
    }
   ],
   "source": [
    "test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea80d3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "path = './models/resnet_gelu.pth'\n",
    "torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15c337c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Step [100], Loss: 0.0047\n",
      "Epoch [1/20], Step [200], Loss: 0.0060\n",
      "Epoch [1/20], Step [300], Loss: 0.0044\n",
      "Epoch [2/20], Step [100], Loss: 0.0036\n",
      "Epoch [2/20], Step [200], Loss: 0.0045\n",
      "Epoch [2/20], Step [300], Loss: 0.0046\n",
      "Epoch [3/20], Step [100], Loss: 0.0048\n",
      "Epoch [3/20], Step [200], Loss: 0.0043\n",
      "Epoch [3/20], Step [300], Loss: 0.0054\n",
      "Epoch [4/20], Step [100], Loss: 0.0052\n",
      "Epoch [4/20], Step [200], Loss: 0.0040\n",
      "Epoch [4/20], Step [300], Loss: 0.0051\n",
      "Epoch [5/20], Step [100], Loss: 0.0043\n",
      "Epoch [5/20], Step [200], Loss: 0.0039\n",
      "Epoch [5/20], Step [300], Loss: 0.0036\n",
      "Epoch [6/20], Step [100], Loss: 0.0044\n",
      "Epoch [6/20], Step [200], Loss: 0.0043\n",
      "Epoch [6/20], Step [300], Loss: 0.0046\n",
      "Epoch [7/20], Step [100], Loss: 0.0048\n",
      "Epoch [7/20], Step [200], Loss: 0.0048\n",
      "Epoch [7/20], Step [300], Loss: 0.0038\n",
      "Epoch [8/20], Step [100], Loss: 0.0047\n",
      "Epoch [8/20], Step [200], Loss: 0.0040\n",
      "Epoch [8/20], Step [300], Loss: 0.0045\n",
      "Epoch [9/20], Step [100], Loss: 0.0053\n",
      "Epoch [9/20], Step [200], Loss: 0.0046\n",
      "Epoch [9/20], Step [300], Loss: 0.0053\n",
      "Epoch [10/20], Step [100], Loss: 0.0050\n",
      "Epoch [10/20], Step [200], Loss: 0.0053\n",
      "Epoch [10/20], Step [300], Loss: 0.0058\n",
      "Epoch [11/20], Step [100], Loss: 0.0055\n",
      "Epoch [11/20], Step [200], Loss: 0.0042\n",
      "Epoch [11/20], Step [300], Loss: 0.0040\n",
      "Epoch [12/20], Step [100], Loss: 0.0047\n",
      "Epoch [12/20], Step [200], Loss: 0.0047\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 12\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(num_epochs)\u001b[0m\n\u001b[1;32m      9\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 12\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     15\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/data/miniconda/envs/torch/lib/python3.10/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/miniconda/envs/torch/lib/python3.10/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/miniconda/envs/torch/lib/python3.10/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(num_epochs=20) #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3a1d94",
   "metadata": {},
   "source": [
    "从损失来看，100 个 epoch 已经基本达到理想状态了。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
