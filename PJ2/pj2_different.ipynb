{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f604ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e16103f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a26c43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0335600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# preprocess\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "497f08e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes * self.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes * self.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * self.expansion)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2494568d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512 * block.expansion, num_classes)\n",
    "    \n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, out.size()[2:])\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "def Net():\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca57b9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "202d3bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(num_epochs=100):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, (images, labels) in enumerate(trainloader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}], Loss: {running_loss/100:.4f}')\n",
    "                running_loss = 0.0\n",
    "        scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aed7eded",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f'Accuracy on test set: {100 * correct / total:.2f}%')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dba5465f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Step [100], Loss: 2.1227\n",
      "Epoch [1/100], Step [200], Loss: 1.7405\n",
      "Epoch [1/100], Step [300], Loss: 1.5755\n",
      "Epoch [2/100], Step [100], Loss: 1.3548\n",
      "Epoch [2/100], Step [200], Loss: 1.2205\n",
      "Epoch [2/100], Step [300], Loss: 1.1379\n",
      "Epoch [3/100], Step [100], Loss: 1.0203\n",
      "Epoch [3/100], Step [200], Loss: 0.9731\n",
      "Epoch [3/100], Step [300], Loss: 0.9374\n",
      "Epoch [4/100], Step [100], Loss: 0.8485\n",
      "Epoch [4/100], Step [200], Loss: 0.8193\n",
      "Epoch [4/100], Step [300], Loss: 0.8016\n",
      "Epoch [5/100], Step [100], Loss: 0.7311\n",
      "Epoch [5/100], Step [200], Loss: 0.7253\n",
      "Epoch [5/100], Step [300], Loss: 0.6892\n",
      "Epoch [6/100], Step [100], Loss: 0.6358\n",
      "Epoch [6/100], Step [200], Loss: 0.6706\n",
      "Epoch [6/100], Step [300], Loss: 0.6178\n",
      "Epoch [7/100], Step [100], Loss: 0.5897\n",
      "Epoch [7/100], Step [200], Loss: 0.5949\n",
      "Epoch [7/100], Step [300], Loss: 0.5831\n",
      "Epoch [8/100], Step [100], Loss: 0.5480\n",
      "Epoch [8/100], Step [200], Loss: 0.5444\n",
      "Epoch [8/100], Step [300], Loss: 0.5350\n",
      "Epoch [9/100], Step [100], Loss: 0.5124\n",
      "Epoch [9/100], Step [200], Loss: 0.5183\n",
      "Epoch [9/100], Step [300], Loss: 0.4928\n",
      "Epoch [10/100], Step [100], Loss: 0.4918\n",
      "Epoch [10/100], Step [200], Loss: 0.4930\n",
      "Epoch [10/100], Step [300], Loss: 0.4813\n",
      "Epoch [11/100], Step [100], Loss: 0.4528\n",
      "Epoch [11/100], Step [200], Loss: 0.4644\n",
      "Epoch [11/100], Step [300], Loss: 0.4637\n",
      "Epoch [12/100], Step [100], Loss: 0.4441\n",
      "Epoch [12/100], Step [200], Loss: 0.4444\n",
      "Epoch [12/100], Step [300], Loss: 0.4397\n",
      "Epoch [13/100], Step [100], Loss: 0.4212\n",
      "Epoch [13/100], Step [200], Loss: 0.4057\n",
      "Epoch [13/100], Step [300], Loss: 0.4249\n",
      "Epoch [14/100], Step [100], Loss: 0.3990\n",
      "Epoch [14/100], Step [200], Loss: 0.4096\n",
      "Epoch [14/100], Step [300], Loss: 0.4079\n",
      "Epoch [15/100], Step [100], Loss: 0.3826\n",
      "Epoch [15/100], Step [200], Loss: 0.3851\n",
      "Epoch [15/100], Step [300], Loss: 0.3795\n",
      "Epoch [16/100], Step [100], Loss: 0.3540\n",
      "Epoch [16/100], Step [200], Loss: 0.3740\n",
      "Epoch [16/100], Step [300], Loss: 0.3683\n",
      "Epoch [17/100], Step [100], Loss: 0.3547\n",
      "Epoch [17/100], Step [200], Loss: 0.3471\n",
      "Epoch [17/100], Step [300], Loss: 0.3591\n",
      "Epoch [18/100], Step [100], Loss: 0.3333\n",
      "Epoch [18/100], Step [200], Loss: 0.3558\n",
      "Epoch [18/100], Step [300], Loss: 0.3354\n",
      "Epoch [19/100], Step [100], Loss: 0.3052\n",
      "Epoch [19/100], Step [200], Loss: 0.3203\n",
      "Epoch [19/100], Step [300], Loss: 0.3354\n",
      "Epoch [20/100], Step [100], Loss: 0.2929\n",
      "Epoch [20/100], Step [200], Loss: 0.3333\n",
      "Epoch [20/100], Step [300], Loss: 0.3213\n",
      "Epoch [21/100], Step [100], Loss: 0.2941\n",
      "Epoch [21/100], Step [200], Loss: 0.3007\n",
      "Epoch [21/100], Step [300], Loss: 0.3019\n",
      "Epoch [22/100], Step [100], Loss: 0.2762\n",
      "Epoch [22/100], Step [200], Loss: 0.3108\n",
      "Epoch [22/100], Step [300], Loss: 0.2994\n",
      "Epoch [23/100], Step [100], Loss: 0.2644\n",
      "Epoch [23/100], Step [200], Loss: 0.2923\n",
      "Epoch [23/100], Step [300], Loss: 0.2956\n",
      "Epoch [24/100], Step [100], Loss: 0.2636\n",
      "Epoch [24/100], Step [200], Loss: 0.2783\n",
      "Epoch [24/100], Step [300], Loss: 0.2762\n",
      "Epoch [25/100], Step [100], Loss: 0.2516\n",
      "Epoch [25/100], Step [200], Loss: 0.2695\n",
      "Epoch [25/100], Step [300], Loss: 0.2692\n",
      "Epoch [26/100], Step [100], Loss: 0.2570\n",
      "Epoch [26/100], Step [200], Loss: 0.2572\n",
      "Epoch [26/100], Step [300], Loss: 0.2605\n",
      "Epoch [27/100], Step [100], Loss: 0.2367\n",
      "Epoch [27/100], Step [200], Loss: 0.2348\n",
      "Epoch [27/100], Step [300], Loss: 0.2566\n",
      "Epoch [28/100], Step [100], Loss: 0.2418\n",
      "Epoch [28/100], Step [200], Loss: 0.2230\n",
      "Epoch [28/100], Step [300], Loss: 0.2358\n",
      "Epoch [29/100], Step [100], Loss: 0.2117\n",
      "Epoch [29/100], Step [200], Loss: 0.2302\n",
      "Epoch [29/100], Step [300], Loss: 0.2423\n",
      "Epoch [30/100], Step [100], Loss: 0.2044\n",
      "Epoch [30/100], Step [200], Loss: 0.2185\n",
      "Epoch [30/100], Step [300], Loss: 0.2324\n",
      "Epoch [31/100], Step [100], Loss: 0.2166\n",
      "Epoch [31/100], Step [200], Loss: 0.2111\n",
      "Epoch [31/100], Step [300], Loss: 0.2361\n",
      "Epoch [32/100], Step [100], Loss: 0.1917\n",
      "Epoch [32/100], Step [200], Loss: 0.2163\n",
      "Epoch [32/100], Step [300], Loss: 0.2064\n",
      "Epoch [33/100], Step [100], Loss: 0.1897\n",
      "Epoch [33/100], Step [200], Loss: 0.2017\n",
      "Epoch [33/100], Step [300], Loss: 0.2031\n",
      "Epoch [34/100], Step [100], Loss: 0.1827\n",
      "Epoch [34/100], Step [200], Loss: 0.1981\n",
      "Epoch [34/100], Step [300], Loss: 0.1990\n",
      "Epoch [35/100], Step [100], Loss: 0.1757\n",
      "Epoch [35/100], Step [200], Loss: 0.1918\n",
      "Epoch [35/100], Step [300], Loss: 0.1880\n",
      "Epoch [36/100], Step [100], Loss: 0.1730\n",
      "Epoch [36/100], Step [200], Loss: 0.1895\n",
      "Epoch [36/100], Step [300], Loss: 0.1862\n",
      "Epoch [37/100], Step [100], Loss: 0.1616\n",
      "Epoch [37/100], Step [200], Loss: 0.1739\n",
      "Epoch [37/100], Step [300], Loss: 0.1865\n",
      "Epoch [38/100], Step [100], Loss: 0.1698\n",
      "Epoch [38/100], Step [200], Loss: 0.1761\n",
      "Epoch [38/100], Step [300], Loss: 0.1759\n",
      "Epoch [39/100], Step [100], Loss: 0.1545\n",
      "Epoch [39/100], Step [200], Loss: 0.1566\n",
      "Epoch [39/100], Step [300], Loss: 0.1709\n",
      "Epoch [40/100], Step [100], Loss: 0.1472\n",
      "Epoch [40/100], Step [200], Loss: 0.1524\n",
      "Epoch [40/100], Step [300], Loss: 0.1584\n",
      "Epoch [41/100], Step [100], Loss: 0.1542\n",
      "Epoch [41/100], Step [200], Loss: 0.1466\n",
      "Epoch [41/100], Step [300], Loss: 0.1479\n",
      "Epoch [42/100], Step [100], Loss: 0.1235\n",
      "Epoch [42/100], Step [200], Loss: 0.1566\n",
      "Epoch [42/100], Step [300], Loss: 0.1580\n",
      "Epoch [43/100], Step [100], Loss: 0.1412\n",
      "Epoch [43/100], Step [200], Loss: 0.1335\n",
      "Epoch [43/100], Step [300], Loss: 0.1380\n",
      "Epoch [44/100], Step [100], Loss: 0.1113\n",
      "Epoch [44/100], Step [200], Loss: 0.1426\n",
      "Epoch [44/100], Step [300], Loss: 0.1483\n",
      "Epoch [45/100], Step [100], Loss: 0.1265\n",
      "Epoch [45/100], Step [200], Loss: 0.1215\n",
      "Epoch [45/100], Step [300], Loss: 0.1320\n",
      "Epoch [46/100], Step [100], Loss: 0.1212\n",
      "Epoch [46/100], Step [200], Loss: 0.1224\n",
      "Epoch [46/100], Step [300], Loss: 0.1388\n",
      "Epoch [47/100], Step [100], Loss: 0.1121\n",
      "Epoch [47/100], Step [200], Loss: 0.1207\n",
      "Epoch [47/100], Step [300], Loss: 0.1205\n",
      "Epoch [48/100], Step [100], Loss: 0.1142\n",
      "Epoch [48/100], Step [200], Loss: 0.1121\n",
      "Epoch [48/100], Step [300], Loss: 0.1113\n",
      "Epoch [49/100], Step [100], Loss: 0.0997\n",
      "Epoch [49/100], Step [200], Loss: 0.1032\n",
      "Epoch [49/100], Step [300], Loss: 0.1155\n",
      "Epoch [50/100], Step [100], Loss: 0.0963\n",
      "Epoch [50/100], Step [200], Loss: 0.1029\n",
      "Epoch [50/100], Step [300], Loss: 0.1160\n",
      "Epoch [51/100], Step [100], Loss: 0.0997\n",
      "Epoch [51/100], Step [200], Loss: 0.1020\n",
      "Epoch [51/100], Step [300], Loss: 0.1044\n",
      "Epoch [52/100], Step [100], Loss: 0.0876\n",
      "Epoch [52/100], Step [200], Loss: 0.0990\n",
      "Epoch [52/100], Step [300], Loss: 0.1024\n",
      "Epoch [53/100], Step [100], Loss: 0.0853\n",
      "Epoch [53/100], Step [200], Loss: 0.0889\n",
      "Epoch [53/100], Step [300], Loss: 0.0872\n",
      "Epoch [54/100], Step [100], Loss: 0.0886\n",
      "Epoch [54/100], Step [200], Loss: 0.0807\n",
      "Epoch [54/100], Step [300], Loss: 0.0883\n",
      "Epoch [55/100], Step [100], Loss: 0.0784\n",
      "Epoch [55/100], Step [200], Loss: 0.0760\n",
      "Epoch [55/100], Step [300], Loss: 0.0820\n",
      "Epoch [56/100], Step [100], Loss: 0.0808\n",
      "Epoch [56/100], Step [200], Loss: 0.0779\n",
      "Epoch [56/100], Step [300], Loss: 0.0754\n",
      "Epoch [57/100], Step [100], Loss: 0.0745\n",
      "Epoch [57/100], Step [200], Loss: 0.0692\n",
      "Epoch [57/100], Step [300], Loss: 0.0790\n",
      "Epoch [58/100], Step [100], Loss: 0.0717\n",
      "Epoch [58/100], Step [200], Loss: 0.0765\n",
      "Epoch [58/100], Step [300], Loss: 0.0804\n",
      "Epoch [59/100], Step [100], Loss: 0.0697\n",
      "Epoch [59/100], Step [200], Loss: 0.0700\n",
      "Epoch [59/100], Step [300], Loss: 0.0728\n",
      "Epoch [60/100], Step [100], Loss: 0.0585\n",
      "Epoch [60/100], Step [200], Loss: 0.0632\n",
      "Epoch [60/100], Step [300], Loss: 0.0684\n",
      "Epoch [61/100], Step [100], Loss: 0.0550\n",
      "Epoch [61/100], Step [200], Loss: 0.0593\n",
      "Epoch [61/100], Step [300], Loss: 0.0581\n",
      "Epoch [62/100], Step [100], Loss: 0.0534\n",
      "Epoch [62/100], Step [200], Loss: 0.0567\n",
      "Epoch [62/100], Step [300], Loss: 0.0636\n",
      "Epoch [63/100], Step [100], Loss: 0.0468\n",
      "Epoch [63/100], Step [200], Loss: 0.0521\n",
      "Epoch [63/100], Step [300], Loss: 0.0527\n",
      "Epoch [64/100], Step [100], Loss: 0.0437\n",
      "Epoch [64/100], Step [200], Loss: 0.0499\n",
      "Epoch [64/100], Step [300], Loss: 0.0508\n",
      "Epoch [65/100], Step [100], Loss: 0.0446\n",
      "Epoch [65/100], Step [200], Loss: 0.0526\n",
      "Epoch [65/100], Step [300], Loss: 0.0499\n",
      "Epoch [66/100], Step [100], Loss: 0.0403\n",
      "Epoch [66/100], Step [200], Loss: 0.0440\n",
      "Epoch [66/100], Step [300], Loss: 0.0424\n",
      "Epoch [67/100], Step [100], Loss: 0.0416\n",
      "Epoch [67/100], Step [200], Loss: 0.0356\n",
      "Epoch [67/100], Step [300], Loss: 0.0423\n",
      "Epoch [68/100], Step [100], Loss: 0.0382\n",
      "Epoch [68/100], Step [200], Loss: 0.0381\n",
      "Epoch [68/100], Step [300], Loss: 0.0374\n",
      "Epoch [69/100], Step [100], Loss: 0.0356\n",
      "Epoch [69/100], Step [200], Loss: 0.0318\n",
      "Epoch [69/100], Step [300], Loss: 0.0353\n",
      "Epoch [70/100], Step [100], Loss: 0.0366\n",
      "Epoch [70/100], Step [200], Loss: 0.0302\n",
      "Epoch [70/100], Step [300], Loss: 0.0343\n",
      "Epoch [71/100], Step [100], Loss: 0.0305\n",
      "Epoch [71/100], Step [200], Loss: 0.0316\n",
      "Epoch [71/100], Step [300], Loss: 0.0305\n",
      "Epoch [72/100], Step [100], Loss: 0.0293\n",
      "Epoch [72/100], Step [200], Loss: 0.0318\n",
      "Epoch [72/100], Step [300], Loss: 0.0313\n",
      "Epoch [73/100], Step [100], Loss: 0.0282\n",
      "Epoch [73/100], Step [200], Loss: 0.0256\n",
      "Epoch [73/100], Step [300], Loss: 0.0304\n",
      "Epoch [74/100], Step [100], Loss: 0.0245\n",
      "Epoch [74/100], Step [200], Loss: 0.0204\n",
      "Epoch [74/100], Step [300], Loss: 0.0210\n",
      "Epoch [75/100], Step [100], Loss: 0.0191\n",
      "Epoch [75/100], Step [200], Loss: 0.0222\n",
      "Epoch [75/100], Step [300], Loss: 0.0216\n",
      "Epoch [76/100], Step [100], Loss: 0.0217\n",
      "Epoch [76/100], Step [200], Loss: 0.0238\n",
      "Epoch [76/100], Step [300], Loss: 0.0205\n",
      "Epoch [77/100], Step [100], Loss: 0.0202\n",
      "Epoch [77/100], Step [200], Loss: 0.0181\n",
      "Epoch [77/100], Step [300], Loss: 0.0193\n",
      "Epoch [78/100], Step [100], Loss: 0.0166\n",
      "Epoch [78/100], Step [200], Loss: 0.0172\n",
      "Epoch [78/100], Step [300], Loss: 0.0159\n",
      "Epoch [79/100], Step [100], Loss: 0.0156\n",
      "Epoch [79/100], Step [200], Loss: 0.0164\n",
      "Epoch [79/100], Step [300], Loss: 0.0166\n",
      "Epoch [80/100], Step [100], Loss: 0.0123\n",
      "Epoch [80/100], Step [200], Loss: 0.0158\n",
      "Epoch [80/100], Step [300], Loss: 0.0136\n",
      "Epoch [81/100], Step [100], Loss: 0.0131\n",
      "Epoch [81/100], Step [200], Loss: 0.0135\n",
      "Epoch [81/100], Step [300], Loss: 0.0138\n",
      "Epoch [82/100], Step [100], Loss: 0.0102\n",
      "Epoch [82/100], Step [200], Loss: 0.0155\n",
      "Epoch [82/100], Step [300], Loss: 0.0139\n",
      "Epoch [83/100], Step [100], Loss: 0.0107\n",
      "Epoch [83/100], Step [200], Loss: 0.0116\n",
      "Epoch [83/100], Step [300], Loss: 0.0121\n",
      "Epoch [84/100], Step [100], Loss: 0.0094\n",
      "Epoch [84/100], Step [200], Loss: 0.0090\n",
      "Epoch [84/100], Step [300], Loss: 0.0103\n",
      "Epoch [85/100], Step [100], Loss: 0.0105\n",
      "Epoch [85/100], Step [200], Loss: 0.0096\n",
      "Epoch [85/100], Step [300], Loss: 0.0106\n",
      "Epoch [86/100], Step [100], Loss: 0.0077\n",
      "Epoch [86/100], Step [200], Loss: 0.0090\n",
      "Epoch [86/100], Step [300], Loss: 0.0098\n",
      "Epoch [87/100], Step [100], Loss: 0.0086\n",
      "Epoch [87/100], Step [200], Loss: 0.0097\n",
      "Epoch [87/100], Step [300], Loss: 0.0072\n",
      "Epoch [88/100], Step [100], Loss: 0.0077\n",
      "Epoch [88/100], Step [200], Loss: 0.0074\n",
      "Epoch [88/100], Step [300], Loss: 0.0082\n",
      "Epoch [89/100], Step [100], Loss: 0.0061\n",
      "Epoch [89/100], Step [200], Loss: 0.0062\n",
      "Epoch [89/100], Step [300], Loss: 0.0063\n",
      "Epoch [90/100], Step [100], Loss: 0.0061\n",
      "Epoch [90/100], Step [200], Loss: 0.0078\n",
      "Epoch [90/100], Step [300], Loss: 0.0059\n",
      "Epoch [91/100], Step [100], Loss: 0.0050\n",
      "Epoch [91/100], Step [200], Loss: 0.0070\n",
      "Epoch [91/100], Step [300], Loss: 0.0063\n",
      "Epoch [92/100], Step [100], Loss: 0.0072\n",
      "Epoch [92/100], Step [200], Loss: 0.0055\n",
      "Epoch [92/100], Step [300], Loss: 0.0051\n",
      "Epoch [93/100], Step [100], Loss: 0.0054\n",
      "Epoch [93/100], Step [200], Loss: 0.0052\n",
      "Epoch [93/100], Step [300], Loss: 0.0053\n",
      "Epoch [94/100], Step [100], Loss: 0.0049\n",
      "Epoch [94/100], Step [200], Loss: 0.0055\n",
      "Epoch [94/100], Step [300], Loss: 0.0061\n",
      "Epoch [95/100], Step [100], Loss: 0.0054\n",
      "Epoch [95/100], Step [200], Loss: 0.0047\n",
      "Epoch [95/100], Step [300], Loss: 0.0054\n",
      "Epoch [96/100], Step [100], Loss: 0.0050\n",
      "Epoch [96/100], Step [200], Loss: 0.0048\n",
      "Epoch [96/100], Step [300], Loss: 0.0048\n",
      "Epoch [97/100], Step [100], Loss: 0.0046\n",
      "Epoch [97/100], Step [200], Loss: 0.0054\n",
      "Epoch [97/100], Step [300], Loss: 0.0035\n",
      "Epoch [98/100], Step [100], Loss: 0.0049\n",
      "Epoch [98/100], Step [200], Loss: 0.0044\n",
      "Epoch [98/100], Step [300], Loss: 0.0048\n",
      "Epoch [99/100], Step [100], Loss: 0.0050\n",
      "Epoch [99/100], Step [200], Loss: 0.0051\n",
      "Epoch [99/100], Step [300], Loss: 0.0037\n",
      "Epoch [100/100], Step [100], Loss: 0.0042\n",
      "Epoch [100/100], Step [200], Loss: 0.0036\n",
      "Epoch [100/100], Step [300], Loss: 0.0039\n"
     ]
    }
   ],
   "source": [
    "train_model(num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "935ce16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 93.43%\n"
     ]
    }
   ],
   "source": [
    "test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea80d3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "path = './model/resnet.pth'\n",
    "torch.save(model.state_dict(), path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
