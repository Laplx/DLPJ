{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f604ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e16103f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a26c43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0335600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# preprocess\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "497f08e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes * self.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes * self.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * self.expansion)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2494568d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Half filters\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 32\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.layer1 = self._make_layer(block, 32, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 64, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 128, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 256, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(256 * block.expansion, num_classes)\n",
    "    \n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, out.size()[2:])\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "def Net():\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "971398bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# smooth for other labels\n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    def __init__(self, smoothing=0.1):\n",
    "        super(LabelSmoothingCrossEntropy, self).__init__()\n",
    "        self.smoothing = smoothing\n",
    "        self.confidence = 1.0 - smoothing\n",
    "    \n",
    "    def forward(self, output, target):\n",
    "        log_probs = F.log_softmax(output, dim=-1)\n",
    "        n_classes = output.size(-1)\n",
    "        true_dist = torch.zeros_like(log_probs)\n",
    "        true_dist.fill_(self.smoothing / (n_classes - 1))\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * log_probs, dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca57b9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to(device)\n",
    "# different loss function\n",
    "criterion = LabelSmoothingCrossEntropy(smoothing=0.1)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "202d3bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(num_epochs=100):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, (images, labels) in enumerate(trainloader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}], Loss: {running_loss/100:.4f}')\n",
    "                running_loss = 0.0\n",
    "        scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aed7eded",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f'Accuracy on test set: {100 * correct / total:.2f}%')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dba5465f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Step [100], Loss: 2.0332\n",
      "Epoch [1/100], Step [200], Loss: 1.7398\n",
      "Epoch [1/100], Step [300], Loss: 1.6021\n",
      "Epoch [2/100], Step [100], Loss: 1.4070\n",
      "Epoch [2/100], Step [200], Loss: 1.3441\n",
      "Epoch [2/100], Step [300], Loss: 1.2976\n",
      "Epoch [3/100], Step [100], Loss: 1.2249\n",
      "Epoch [3/100], Step [200], Loss: 1.1861\n",
      "Epoch [3/100], Step [300], Loss: 1.1815\n",
      "Epoch [4/100], Step [100], Loss: 1.1091\n",
      "Epoch [4/100], Step [200], Loss: 1.1101\n",
      "Epoch [4/100], Step [300], Loss: 1.0809\n",
      "Epoch [5/100], Step [100], Loss: 1.0385\n",
      "Epoch [5/100], Step [200], Loss: 1.0326\n",
      "Epoch [5/100], Step [300], Loss: 1.0329\n",
      "Epoch [6/100], Step [100], Loss: 0.9906\n",
      "Epoch [6/100], Step [200], Loss: 0.9999\n",
      "Epoch [6/100], Step [300], Loss: 0.9861\n",
      "Epoch [7/100], Step [100], Loss: 0.9671\n",
      "Epoch [7/100], Step [200], Loss: 0.9637\n",
      "Epoch [7/100], Step [300], Loss: 0.9595\n",
      "Epoch [8/100], Step [100], Loss: 0.9400\n",
      "Epoch [8/100], Step [200], Loss: 0.9264\n",
      "Epoch [8/100], Step [300], Loss: 0.9305\n",
      "Epoch [9/100], Step [100], Loss: 0.9092\n",
      "Epoch [9/100], Step [200], Loss: 0.9151\n",
      "Epoch [9/100], Step [300], Loss: 0.9193\n",
      "Epoch [10/100], Step [100], Loss: 0.8889\n",
      "Epoch [10/100], Step [200], Loss: 0.8982\n",
      "Epoch [10/100], Step [300], Loss: 0.9003\n",
      "Epoch [11/100], Step [100], Loss: 0.8754\n",
      "Epoch [11/100], Step [200], Loss: 0.8876\n",
      "Epoch [11/100], Step [300], Loss: 0.8857\n",
      "Epoch [12/100], Step [100], Loss: 0.8505\n",
      "Epoch [12/100], Step [200], Loss: 0.8652\n",
      "Epoch [12/100], Step [300], Loss: 0.8730\n",
      "Epoch [13/100], Step [100], Loss: 0.8530\n",
      "Epoch [13/100], Step [200], Loss: 0.8461\n",
      "Epoch [13/100], Step [300], Loss: 0.8467\n",
      "Epoch [14/100], Step [100], Loss: 0.8322\n",
      "Epoch [14/100], Step [200], Loss: 0.8480\n",
      "Epoch [14/100], Step [300], Loss: 0.8397\n",
      "Epoch [15/100], Step [100], Loss: 0.8257\n",
      "Epoch [15/100], Step [200], Loss: 0.8328\n",
      "Epoch [15/100], Step [300], Loss: 0.8175\n",
      "Epoch [16/100], Step [100], Loss: 0.8121\n",
      "Epoch [16/100], Step [200], Loss: 0.8163\n",
      "Epoch [16/100], Step [300], Loss: 0.8179\n",
      "Epoch [17/100], Step [100], Loss: 0.8074\n",
      "Epoch [17/100], Step [200], Loss: 0.8125\n",
      "Epoch [17/100], Step [300], Loss: 0.8135\n",
      "Epoch [18/100], Step [100], Loss: 0.7968\n",
      "Epoch [18/100], Step [200], Loss: 0.7967\n",
      "Epoch [18/100], Step [300], Loss: 0.8053\n",
      "Epoch [19/100], Step [100], Loss: 0.7877\n",
      "Epoch [19/100], Step [200], Loss: 0.7937\n",
      "Epoch [19/100], Step [300], Loss: 0.7920\n",
      "Epoch [20/100], Step [100], Loss: 0.7815\n",
      "Epoch [20/100], Step [200], Loss: 0.7856\n",
      "Epoch [20/100], Step [300], Loss: 0.7842\n",
      "Epoch [21/100], Step [100], Loss: 0.7675\n",
      "Epoch [21/100], Step [200], Loss: 0.7821\n",
      "Epoch [21/100], Step [300], Loss: 0.7836\n",
      "Epoch [22/100], Step [100], Loss: 0.7573\n",
      "Epoch [22/100], Step [200], Loss: 0.7707\n",
      "Epoch [22/100], Step [300], Loss: 0.7777\n",
      "Epoch [23/100], Step [100], Loss: 0.7502\n",
      "Epoch [23/100], Step [200], Loss: 0.7645\n",
      "Epoch [23/100], Step [300], Loss: 0.7699\n",
      "Epoch [24/100], Step [100], Loss: 0.7533\n",
      "Epoch [24/100], Step [200], Loss: 0.7587\n",
      "Epoch [24/100], Step [300], Loss: 0.7587\n",
      "Epoch [25/100], Step [100], Loss: 0.7407\n",
      "Epoch [25/100], Step [200], Loss: 0.7617\n",
      "Epoch [25/100], Step [300], Loss: 0.7524\n",
      "Epoch [26/100], Step [100], Loss: 0.7505\n",
      "Epoch [26/100], Step [200], Loss: 0.7455\n",
      "Epoch [26/100], Step [300], Loss: 0.7456\n",
      "Epoch [27/100], Step [100], Loss: 0.7327\n",
      "Epoch [27/100], Step [200], Loss: 0.7326\n",
      "Epoch [27/100], Step [300], Loss: 0.7456\n",
      "Epoch [28/100], Step [100], Loss: 0.7331\n",
      "Epoch [28/100], Step [200], Loss: 0.7336\n",
      "Epoch [28/100], Step [300], Loss: 0.7343\n",
      "Epoch [29/100], Step [100], Loss: 0.7210\n",
      "Epoch [29/100], Step [200], Loss: 0.7363\n",
      "Epoch [29/100], Step [300], Loss: 0.7388\n",
      "Epoch [30/100], Step [100], Loss: 0.7232\n",
      "Epoch [30/100], Step [200], Loss: 0.7296\n",
      "Epoch [30/100], Step [300], Loss: 0.7234\n",
      "Epoch [31/100], Step [100], Loss: 0.7150\n",
      "Epoch [31/100], Step [200], Loss: 0.7193\n",
      "Epoch [31/100], Step [300], Loss: 0.7307\n",
      "Epoch [32/100], Step [100], Loss: 0.7111\n",
      "Epoch [32/100], Step [200], Loss: 0.7285\n",
      "Epoch [32/100], Step [300], Loss: 0.7197\n",
      "Epoch [33/100], Step [100], Loss: 0.7044\n",
      "Epoch [33/100], Step [200], Loss: 0.7100\n",
      "Epoch [33/100], Step [300], Loss: 0.7175\n",
      "Epoch [34/100], Step [100], Loss: 0.7009\n",
      "Epoch [34/100], Step [200], Loss: 0.7083\n",
      "Epoch [34/100], Step [300], Loss: 0.7136\n",
      "Epoch [35/100], Step [100], Loss: 0.7030\n",
      "Epoch [35/100], Step [200], Loss: 0.6980\n",
      "Epoch [35/100], Step [300], Loss: 0.7100\n",
      "Epoch [36/100], Step [100], Loss: 0.6978\n",
      "Epoch [36/100], Step [200], Loss: 0.7043\n",
      "Epoch [36/100], Step [300], Loss: 0.7095\n",
      "Epoch [37/100], Step [100], Loss: 0.6860\n",
      "Epoch [37/100], Step [200], Loss: 0.6906\n",
      "Epoch [37/100], Step [300], Loss: 0.6974\n",
      "Epoch [38/100], Step [100], Loss: 0.6810\n",
      "Epoch [38/100], Step [200], Loss: 0.6906\n",
      "Epoch [38/100], Step [300], Loss: 0.6925\n",
      "Epoch [39/100], Step [100], Loss: 0.6836\n",
      "Epoch [39/100], Step [200], Loss: 0.6922\n",
      "Epoch [39/100], Step [300], Loss: 0.6787\n",
      "Epoch [40/100], Step [100], Loss: 0.6753\n",
      "Epoch [40/100], Step [200], Loss: 0.6838\n",
      "Epoch [40/100], Step [300], Loss: 0.6933\n",
      "Epoch [41/100], Step [100], Loss: 0.6730\n",
      "Epoch [41/100], Step [200], Loss: 0.6723\n",
      "Epoch [41/100], Step [300], Loss: 0.6801\n",
      "Epoch [42/100], Step [100], Loss: 0.6689\n",
      "Epoch [42/100], Step [200], Loss: 0.6750\n",
      "Epoch [42/100], Step [300], Loss: 0.6797\n",
      "Epoch [43/100], Step [100], Loss: 0.6620\n",
      "Epoch [43/100], Step [200], Loss: 0.6631\n",
      "Epoch [43/100], Step [300], Loss: 0.6683\n",
      "Epoch [44/100], Step [100], Loss: 0.6653\n",
      "Epoch [44/100], Step [200], Loss: 0.6711\n",
      "Epoch [44/100], Step [300], Loss: 0.6682\n",
      "Epoch [45/100], Step [100], Loss: 0.6493\n",
      "Epoch [45/100], Step [200], Loss: 0.6636\n",
      "Epoch [45/100], Step [300], Loss: 0.6696\n",
      "Epoch [46/100], Step [100], Loss: 0.6538\n",
      "Epoch [46/100], Step [200], Loss: 0.6578\n",
      "Epoch [46/100], Step [300], Loss: 0.6625\n",
      "Epoch [47/100], Step [100], Loss: 0.6531\n",
      "Epoch [47/100], Step [200], Loss: 0.6529\n",
      "Epoch [47/100], Step [300], Loss: 0.6593\n",
      "Epoch [48/100], Step [100], Loss: 0.6467\n",
      "Epoch [48/100], Step [200], Loss: 0.6463\n",
      "Epoch [48/100], Step [300], Loss: 0.6598\n",
      "Epoch [49/100], Step [100], Loss: 0.6424\n",
      "Epoch [49/100], Step [200], Loss: 0.6434\n",
      "Epoch [49/100], Step [300], Loss: 0.6567\n",
      "Epoch [50/100], Step [100], Loss: 0.6368\n",
      "Epoch [50/100], Step [200], Loss: 0.6444\n",
      "Epoch [50/100], Step [300], Loss: 0.6434\n",
      "Epoch [51/100], Step [100], Loss: 0.6343\n",
      "Epoch [51/100], Step [200], Loss: 0.6382\n",
      "Epoch [51/100], Step [300], Loss: 0.6397\n",
      "Epoch [52/100], Step [100], Loss: 0.6307\n",
      "Epoch [52/100], Step [200], Loss: 0.6238\n",
      "Epoch [52/100], Step [300], Loss: 0.6414\n",
      "Epoch [53/100], Step [100], Loss: 0.6264\n",
      "Epoch [53/100], Step [200], Loss: 0.6373\n",
      "Epoch [53/100], Step [300], Loss: 0.6355\n",
      "Epoch [54/100], Step [100], Loss: 0.6231\n",
      "Epoch [54/100], Step [200], Loss: 0.6298\n",
      "Epoch [54/100], Step [300], Loss: 0.6386\n",
      "Epoch [55/100], Step [100], Loss: 0.6219\n",
      "Epoch [55/100], Step [200], Loss: 0.6247\n",
      "Epoch [55/100], Step [300], Loss: 0.6222\n",
      "Epoch [56/100], Step [100], Loss: 0.6237\n",
      "Epoch [56/100], Step [200], Loss: 0.6211\n",
      "Epoch [56/100], Step [300], Loss: 0.6220\n",
      "Epoch [57/100], Step [100], Loss: 0.6144\n",
      "Epoch [57/100], Step [200], Loss: 0.6180\n",
      "Epoch [57/100], Step [300], Loss: 0.6153\n",
      "Epoch [58/100], Step [100], Loss: 0.6092\n",
      "Epoch [58/100], Step [200], Loss: 0.6163\n",
      "Epoch [58/100], Step [300], Loss: 0.6191\n",
      "Epoch [59/100], Step [100], Loss: 0.6150\n",
      "Epoch [59/100], Step [200], Loss: 0.6101\n",
      "Epoch [59/100], Step [300], Loss: 0.6115\n",
      "Epoch [60/100], Step [100], Loss: 0.6050\n",
      "Epoch [60/100], Step [200], Loss: 0.6077\n",
      "Epoch [60/100], Step [300], Loss: 0.6124\n",
      "Epoch [61/100], Step [100], Loss: 0.6034\n",
      "Epoch [61/100], Step [200], Loss: 0.6023\n",
      "Epoch [61/100], Step [300], Loss: 0.6061\n",
      "Epoch [62/100], Step [100], Loss: 0.6072\n",
      "Epoch [62/100], Step [200], Loss: 0.6066\n",
      "Epoch [62/100], Step [300], Loss: 0.6035\n",
      "Epoch [63/100], Step [100], Loss: 0.5941\n",
      "Epoch [63/100], Step [200], Loss: 0.5977\n",
      "Epoch [63/100], Step [300], Loss: 0.6004\n",
      "Epoch [64/100], Step [100], Loss: 0.5970\n",
      "Epoch [64/100], Step [200], Loss: 0.5969\n",
      "Epoch [64/100], Step [300], Loss: 0.6005\n",
      "Epoch [65/100], Step [100], Loss: 0.5956\n",
      "Epoch [65/100], Step [200], Loss: 0.5943\n",
      "Epoch [65/100], Step [300], Loss: 0.6008\n",
      "Epoch [66/100], Step [100], Loss: 0.5899\n",
      "Epoch [66/100], Step [200], Loss: 0.5893\n",
      "Epoch [66/100], Step [300], Loss: 0.5938\n",
      "Epoch [67/100], Step [100], Loss: 0.5905\n",
      "Epoch [67/100], Step [200], Loss: 0.5887\n",
      "Epoch [67/100], Step [300], Loss: 0.5913\n",
      "Epoch [68/100], Step [100], Loss: 0.5876\n",
      "Epoch [68/100], Step [200], Loss: 0.5884\n",
      "Epoch [68/100], Step [300], Loss: 0.5885\n",
      "Epoch [69/100], Step [100], Loss: 0.5779\n",
      "Epoch [69/100], Step [200], Loss: 0.5840\n",
      "Epoch [69/100], Step [300], Loss: 0.5831\n",
      "Epoch [70/100], Step [100], Loss: 0.5819\n",
      "Epoch [70/100], Step [200], Loss: 0.5791\n",
      "Epoch [70/100], Step [300], Loss: 0.5839\n",
      "Epoch [71/100], Step [100], Loss: 0.5790\n",
      "Epoch [71/100], Step [200], Loss: 0.5748\n",
      "Epoch [71/100], Step [300], Loss: 0.5813\n",
      "Epoch [72/100], Step [100], Loss: 0.5782\n",
      "Epoch [72/100], Step [200], Loss: 0.5788\n",
      "Epoch [72/100], Step [300], Loss: 0.5768\n",
      "Epoch [73/100], Step [100], Loss: 0.5788\n",
      "Epoch [73/100], Step [200], Loss: 0.5764\n",
      "Epoch [73/100], Step [300], Loss: 0.5762\n",
      "Epoch [74/100], Step [100], Loss: 0.5728\n",
      "Epoch [74/100], Step [200], Loss: 0.5753\n",
      "Epoch [74/100], Step [300], Loss: 0.5774\n",
      "Epoch [75/100], Step [100], Loss: 0.5713\n",
      "Epoch [75/100], Step [200], Loss: 0.5731\n",
      "Epoch [75/100], Step [300], Loss: 0.5737\n",
      "Epoch [76/100], Step [100], Loss: 0.5679\n",
      "Epoch [76/100], Step [200], Loss: 0.5671\n",
      "Epoch [76/100], Step [300], Loss: 0.5693\n",
      "Epoch [77/100], Step [100], Loss: 0.5666\n",
      "Epoch [77/100], Step [200], Loss: 0.5684\n",
      "Epoch [77/100], Step [300], Loss: 0.5701\n",
      "Epoch [78/100], Step [100], Loss: 0.5653\n",
      "Epoch [78/100], Step [200], Loss: 0.5653\n",
      "Epoch [78/100], Step [300], Loss: 0.5676\n",
      "Epoch [79/100], Step [100], Loss: 0.5636\n",
      "Epoch [79/100], Step [200], Loss: 0.5647\n",
      "Epoch [79/100], Step [300], Loss: 0.5652\n",
      "Epoch [80/100], Step [100], Loss: 0.5630\n",
      "Epoch [80/100], Step [200], Loss: 0.5645\n",
      "Epoch [80/100], Step [300], Loss: 0.5627\n",
      "Epoch [81/100], Step [100], Loss: 0.5627\n",
      "Epoch [81/100], Step [200], Loss: 0.5607\n",
      "Epoch [81/100], Step [300], Loss: 0.5604\n",
      "Epoch [82/100], Step [100], Loss: 0.5606\n",
      "Epoch [82/100], Step [200], Loss: 0.5596\n",
      "Epoch [82/100], Step [300], Loss: 0.5616\n",
      "Epoch [83/100], Step [100], Loss: 0.5597\n",
      "Epoch [83/100], Step [200], Loss: 0.5604\n",
      "Epoch [83/100], Step [300], Loss: 0.5587\n",
      "Epoch [84/100], Step [100], Loss: 0.5561\n",
      "Epoch [84/100], Step [200], Loss: 0.5581\n",
      "Epoch [84/100], Step [300], Loss: 0.5589\n",
      "Epoch [85/100], Step [100], Loss: 0.5560\n",
      "Epoch [85/100], Step [200], Loss: 0.5564\n",
      "Epoch [85/100], Step [300], Loss: 0.5587\n",
      "Epoch [86/100], Step [100], Loss: 0.5562\n",
      "Epoch [86/100], Step [200], Loss: 0.5576\n",
      "Epoch [86/100], Step [300], Loss: 0.5560\n",
      "Epoch [87/100], Step [100], Loss: 0.5579\n",
      "Epoch [87/100], Step [200], Loss: 0.5569\n",
      "Epoch [87/100], Step [300], Loss: 0.5566\n",
      "Epoch [88/100], Step [100], Loss: 0.5582\n",
      "Epoch [88/100], Step [200], Loss: 0.5556\n",
      "Epoch [88/100], Step [300], Loss: 0.5563\n",
      "Epoch [89/100], Step [100], Loss: 0.5553\n",
      "Epoch [89/100], Step [200], Loss: 0.5548\n",
      "Epoch [89/100], Step [300], Loss: 0.5546\n",
      "Epoch [90/100], Step [100], Loss: 0.5535\n",
      "Epoch [90/100], Step [200], Loss: 0.5547\n",
      "Epoch [90/100], Step [300], Loss: 0.5540\n",
      "Epoch [91/100], Step [100], Loss: 0.5540\n",
      "Epoch [91/100], Step [200], Loss: 0.5542\n",
      "Epoch [91/100], Step [300], Loss: 0.5534\n",
      "Epoch [92/100], Step [100], Loss: 0.5532\n",
      "Epoch [92/100], Step [200], Loss: 0.5549\n",
      "Epoch [92/100], Step [300], Loss: 0.5533\n",
      "Epoch [93/100], Step [100], Loss: 0.5533\n",
      "Epoch [93/100], Step [200], Loss: 0.5535\n",
      "Epoch [93/100], Step [300], Loss: 0.5539\n",
      "Epoch [94/100], Step [100], Loss: 0.5528\n",
      "Epoch [94/100], Step [200], Loss: 0.5526\n",
      "Epoch [94/100], Step [300], Loss: 0.5518\n",
      "Epoch [95/100], Step [100], Loss: 0.5534\n",
      "Epoch [95/100], Step [200], Loss: 0.5525\n",
      "Epoch [95/100], Step [300], Loss: 0.5521\n",
      "Epoch [96/100], Step [100], Loss: 0.5529\n",
      "Epoch [96/100], Step [200], Loss: 0.5530\n",
      "Epoch [96/100], Step [300], Loss: 0.5523\n",
      "Epoch [97/100], Step [100], Loss: 0.5516\n",
      "Epoch [97/100], Step [200], Loss: 0.5520\n",
      "Epoch [97/100], Step [300], Loss: 0.5535\n",
      "Epoch [98/100], Step [100], Loss: 0.5510\n",
      "Epoch [98/100], Step [200], Loss: 0.5522\n",
      "Epoch [98/100], Step [300], Loss: 0.5528\n",
      "Epoch [99/100], Step [100], Loss: 0.5521\n",
      "Epoch [99/100], Step [200], Loss: 0.5515\n",
      "Epoch [99/100], Step [300], Loss: 0.5506\n",
      "Epoch [100/100], Step [100], Loss: 0.5515\n",
      "Epoch [100/100], Step [200], Loss: 0.5513\n",
      "Epoch [100/100], Step [300], Loss: 0.5513\n"
     ]
    }
   ],
   "source": [
    "train_model(num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "935ce16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 93.52%\n"
     ]
    }
   ],
   "source": [
    "test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea80d3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "path = './models/resnet_labelsmooth.pth'\n",
    "torch.save(model.state_dict(), path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
