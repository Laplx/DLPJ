{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f604ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e16103f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a26c43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0335600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# preprocess\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "497f08e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes * self.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes * self.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * self.expansion)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2494568d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Half filters\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 32\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.layer1 = self._make_layer(block, 32, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 64, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 128, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 256, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(256 * block.expansion, num_classes)\n",
    "    \n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, out.size()[2:])\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "def Net():\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca57b9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "202d3bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(num_epochs=100):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, (images, labels) in enumerate(trainloader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}], Loss: {running_loss/100:.4f}')\n",
    "                running_loss = 0.0\n",
    "        scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aed7eded",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f'Accuracy on test set: {100 * correct / total:.2f}%')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dba5465f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/75], Step [100], Loss: 0.9345\n",
      "Epoch [1/75], Step [200], Loss: 0.8847\n",
      "Epoch [1/75], Step [300], Loss: 0.8561\n",
      "Epoch [2/75], Step [100], Loss: 0.7888\n",
      "Epoch [2/75], Step [200], Loss: 0.7473\n",
      "Epoch [2/75], Step [300], Loss: 0.7210\n",
      "Epoch [3/75], Step [100], Loss: 0.6665\n",
      "Epoch [3/75], Step [200], Loss: 0.6501\n",
      "Epoch [3/75], Step [300], Loss: 0.6460\n",
      "Epoch [4/75], Step [100], Loss: 0.5732\n",
      "Epoch [4/75], Step [200], Loss: 0.5845\n",
      "Epoch [4/75], Step [300], Loss: 0.5715\n",
      "Epoch [5/75], Step [100], Loss: 0.5406\n",
      "Epoch [5/75], Step [200], Loss: 0.5396\n",
      "Epoch [5/75], Step [300], Loss: 0.5403\n",
      "Epoch [6/75], Step [100], Loss: 0.4962\n",
      "Epoch [6/75], Step [200], Loss: 0.4959\n",
      "Epoch [6/75], Step [300], Loss: 0.5098\n",
      "Epoch [7/75], Step [100], Loss: 0.4838\n",
      "Epoch [7/75], Step [200], Loss: 0.4614\n",
      "Epoch [7/75], Step [300], Loss: 0.4585\n",
      "Epoch [8/75], Step [100], Loss: 0.4400\n",
      "Epoch [8/75], Step [200], Loss: 0.4366\n",
      "Epoch [8/75], Step [300], Loss: 0.4384\n",
      "Epoch [9/75], Step [100], Loss: 0.4300\n",
      "Epoch [9/75], Step [200], Loss: 0.4239\n",
      "Epoch [9/75], Step [300], Loss: 0.4200\n",
      "Epoch [10/75], Step [100], Loss: 0.3935\n",
      "Epoch [10/75], Step [200], Loss: 0.4078\n",
      "Epoch [10/75], Step [300], Loss: 0.4127\n",
      "Epoch [11/75], Step [100], Loss: 0.3760\n",
      "Epoch [11/75], Step [200], Loss: 0.3931\n",
      "Epoch [11/75], Step [300], Loss: 0.3761\n",
      "Epoch [12/75], Step [100], Loss: 0.3652\n",
      "Epoch [12/75], Step [200], Loss: 0.3785\n",
      "Epoch [12/75], Step [300], Loss: 0.3699\n",
      "Epoch [13/75], Step [100], Loss: 0.3575\n",
      "Epoch [13/75], Step [200], Loss: 0.3674\n",
      "Epoch [13/75], Step [300], Loss: 0.3544\n",
      "Epoch [14/75], Step [100], Loss: 0.3335\n",
      "Epoch [14/75], Step [200], Loss: 0.3410\n",
      "Epoch [14/75], Step [300], Loss: 0.3433\n",
      "Epoch [15/75], Step [100], Loss: 0.3145\n",
      "Epoch [15/75], Step [200], Loss: 0.3436\n",
      "Epoch [15/75], Step [300], Loss: 0.3393\n",
      "Epoch [16/75], Step [100], Loss: 0.3062\n",
      "Epoch [16/75], Step [200], Loss: 0.3193\n",
      "Epoch [16/75], Step [300], Loss: 0.3080\n",
      "Epoch [17/75], Step [100], Loss: 0.2934\n",
      "Epoch [17/75], Step [200], Loss: 0.3127\n",
      "Epoch [17/75], Step [300], Loss: 0.3198\n",
      "Epoch [18/75], Step [100], Loss: 0.2911\n",
      "Epoch [18/75], Step [200], Loss: 0.2850\n",
      "Epoch [18/75], Step [300], Loss: 0.3110\n",
      "Epoch [19/75], Step [100], Loss: 0.2810\n",
      "Epoch [19/75], Step [200], Loss: 0.2901\n",
      "Epoch [19/75], Step [300], Loss: 0.2899\n",
      "Epoch [20/75], Step [100], Loss: 0.2721\n",
      "Epoch [20/75], Step [200], Loss: 0.2811\n",
      "Epoch [20/75], Step [300], Loss: 0.2779\n",
      "Epoch [21/75], Step [100], Loss: 0.2610\n",
      "Epoch [21/75], Step [200], Loss: 0.2613\n",
      "Epoch [21/75], Step [300], Loss: 0.2641\n",
      "Epoch [22/75], Step [100], Loss: 0.2566\n",
      "Epoch [22/75], Step [200], Loss: 0.2551\n",
      "Epoch [22/75], Step [300], Loss: 0.2561\n",
      "Epoch [23/75], Step [100], Loss: 0.2441\n",
      "Epoch [23/75], Step [200], Loss: 0.2507\n",
      "Epoch [23/75], Step [300], Loss: 0.2630\n",
      "Epoch [24/75], Step [100], Loss: 0.2399\n",
      "Epoch [24/75], Step [200], Loss: 0.2421\n",
      "Epoch [24/75], Step [300], Loss: 0.2346\n",
      "Epoch [25/75], Step [100], Loss: 0.2307\n",
      "Epoch [25/75], Step [200], Loss: 0.2354\n",
      "Epoch [25/75], Step [300], Loss: 0.2464\n",
      "Epoch [26/75], Step [100], Loss: 0.2260\n",
      "Epoch [26/75], Step [200], Loss: 0.2159\n",
      "Epoch [26/75], Step [300], Loss: 0.2342\n",
      "Epoch [27/75], Step [100], Loss: 0.2097\n",
      "Epoch [27/75], Step [200], Loss: 0.2288\n",
      "Epoch [27/75], Step [300], Loss: 0.2190\n",
      "Epoch [28/75], Step [100], Loss: 0.2003\n",
      "Epoch [28/75], Step [200], Loss: 0.2152\n",
      "Epoch [28/75], Step [300], Loss: 0.2231\n",
      "Epoch [29/75], Step [100], Loss: 0.1957\n",
      "Epoch [29/75], Step [200], Loss: 0.2114\n",
      "Epoch [29/75], Step [300], Loss: 0.2103\n",
      "Epoch [30/75], Step [100], Loss: 0.1934\n",
      "Epoch [30/75], Step [200], Loss: 0.1935\n",
      "Epoch [30/75], Step [300], Loss: 0.2141\n",
      "Epoch [31/75], Step [100], Loss: 0.1847\n",
      "Epoch [31/75], Step [200], Loss: 0.2012\n",
      "Epoch [31/75], Step [300], Loss: 0.1995\n",
      "Epoch [32/75], Step [100], Loss: 0.1722\n",
      "Epoch [32/75], Step [200], Loss: 0.1803\n",
      "Epoch [32/75], Step [300], Loss: 0.1956\n",
      "Epoch [33/75], Step [100], Loss: 0.1813\n",
      "Epoch [33/75], Step [200], Loss: 0.1784\n",
      "Epoch [33/75], Step [300], Loss: 0.1926\n",
      "Epoch [34/75], Step [100], Loss: 0.1762\n",
      "Epoch [34/75], Step [200], Loss: 0.1746\n",
      "Epoch [34/75], Step [300], Loss: 0.1831\n",
      "Epoch [35/75], Step [100], Loss: 0.1633\n",
      "Epoch [35/75], Step [200], Loss: 0.1736\n",
      "Epoch [35/75], Step [300], Loss: 0.1783\n",
      "Epoch [36/75], Step [100], Loss: 0.1600\n",
      "Epoch [36/75], Step [200], Loss: 0.1735\n",
      "Epoch [36/75], Step [300], Loss: 0.1652\n",
      "Epoch [37/75], Step [100], Loss: 0.1568\n",
      "Epoch [37/75], Step [200], Loss: 0.1601\n",
      "Epoch [37/75], Step [300], Loss: 0.1674\n",
      "Epoch [38/75], Step [100], Loss: 0.1431\n",
      "Epoch [38/75], Step [200], Loss: 0.1554\n",
      "Epoch [38/75], Step [300], Loss: 0.1519\n",
      "Epoch [39/75], Step [100], Loss: 0.1490\n",
      "Epoch [39/75], Step [200], Loss: 0.1471\n",
      "Epoch [39/75], Step [300], Loss: 0.1571\n",
      "Epoch [40/75], Step [100], Loss: 0.1362\n",
      "Epoch [40/75], Step [200], Loss: 0.1509\n",
      "Epoch [40/75], Step [300], Loss: 0.1556\n",
      "Epoch [41/75], Step [100], Loss: 0.1278\n",
      "Epoch [41/75], Step [200], Loss: 0.1424\n",
      "Epoch [41/75], Step [300], Loss: 0.1467\n",
      "Epoch [42/75], Step [100], Loss: 0.1250\n",
      "Epoch [42/75], Step [200], Loss: 0.1392\n",
      "Epoch [42/75], Step [300], Loss: 0.1391\n",
      "Epoch [43/75], Step [100], Loss: 0.1226\n",
      "Epoch [43/75], Step [200], Loss: 0.1331\n",
      "Epoch [43/75], Step [300], Loss: 0.1317\n",
      "Epoch [44/75], Step [100], Loss: 0.1231\n",
      "Epoch [44/75], Step [200], Loss: 0.1225\n",
      "Epoch [44/75], Step [300], Loss: 0.1220\n",
      "Epoch [45/75], Step [100], Loss: 0.1200\n",
      "Epoch [45/75], Step [200], Loss: 0.1229\n",
      "Epoch [45/75], Step [300], Loss: 0.1184\n",
      "Epoch [46/75], Step [100], Loss: 0.1133\n",
      "Epoch [46/75], Step [200], Loss: 0.1152\n",
      "Epoch [46/75], Step [300], Loss: 0.1278\n",
      "Epoch [47/75], Step [100], Loss: 0.1014\n",
      "Epoch [47/75], Step [200], Loss: 0.1146\n",
      "Epoch [47/75], Step [300], Loss: 0.1237\n",
      "Epoch [48/75], Step [100], Loss: 0.1062\n",
      "Epoch [48/75], Step [200], Loss: 0.1018\n",
      "Epoch [48/75], Step [300], Loss: 0.1065\n",
      "Epoch [49/75], Step [100], Loss: 0.0946\n",
      "Epoch [49/75], Step [200], Loss: 0.1125\n",
      "Epoch [49/75], Step [300], Loss: 0.1035\n",
      "Epoch [50/75], Step [100], Loss: 0.0900\n",
      "Epoch [50/75], Step [200], Loss: 0.0919\n",
      "Epoch [50/75], Step [300], Loss: 0.1021\n",
      "Epoch [51/75], Step [100], Loss: 0.0864\n",
      "Epoch [51/75], Step [200], Loss: 0.0831\n",
      "Epoch [51/75], Step [300], Loss: 0.1057\n",
      "Epoch [52/75], Step [100], Loss: 0.0818\n",
      "Epoch [52/75], Step [200], Loss: 0.0974\n",
      "Epoch [52/75], Step [300], Loss: 0.0915\n",
      "Epoch [53/75], Step [100], Loss: 0.0795\n",
      "Epoch [53/75], Step [200], Loss: 0.0892\n",
      "Epoch [53/75], Step [300], Loss: 0.0921\n",
      "Epoch [54/75], Step [100], Loss: 0.0799\n",
      "Epoch [54/75], Step [200], Loss: 0.0829\n",
      "Epoch [54/75], Step [300], Loss: 0.0830\n",
      "Epoch [55/75], Step [100], Loss: 0.0818\n",
      "Epoch [55/75], Step [200], Loss: 0.0743\n",
      "Epoch [55/75], Step [300], Loss: 0.0740\n",
      "Epoch [56/75], Step [100], Loss: 0.0678\n",
      "Epoch [56/75], Step [200], Loss: 0.0759\n",
      "Epoch [56/75], Step [300], Loss: 0.0766\n",
      "Epoch [57/75], Step [100], Loss: 0.0694\n",
      "Epoch [57/75], Step [200], Loss: 0.0639\n",
      "Epoch [57/75], Step [300], Loss: 0.0821\n",
      "Epoch [58/75], Step [100], Loss: 0.0623\n",
      "Epoch [58/75], Step [200], Loss: 0.0624\n",
      "Epoch [58/75], Step [300], Loss: 0.0616\n",
      "Epoch [59/75], Step [100], Loss: 0.0660\n",
      "Epoch [59/75], Step [200], Loss: 0.0627\n",
      "Epoch [59/75], Step [300], Loss: 0.0634\n",
      "Epoch [60/75], Step [100], Loss: 0.0548\n",
      "Epoch [60/75], Step [200], Loss: 0.0632\n",
      "Epoch [60/75], Step [300], Loss: 0.0616\n",
      "Epoch [61/75], Step [100], Loss: 0.0526\n",
      "Epoch [61/75], Step [200], Loss: 0.0551\n",
      "Epoch [61/75], Step [300], Loss: 0.0594\n",
      "Epoch [62/75], Step [100], Loss: 0.0613\n",
      "Epoch [62/75], Step [200], Loss: 0.0470\n",
      "Epoch [62/75], Step [300], Loss: 0.0565\n",
      "Epoch [63/75], Step [100], Loss: 0.0475\n",
      "Epoch [63/75], Step [200], Loss: 0.0503\n",
      "Epoch [63/75], Step [300], Loss: 0.0509\n",
      "Epoch [64/75], Step [100], Loss: 0.0458\n",
      "Epoch [64/75], Step [200], Loss: 0.0438\n",
      "Epoch [64/75], Step [300], Loss: 0.0521\n",
      "Epoch [65/75], Step [100], Loss: 0.0432\n",
      "Epoch [65/75], Step [200], Loss: 0.0400\n",
      "Epoch [65/75], Step [300], Loss: 0.0416\n",
      "Epoch [66/75], Step [100], Loss: 0.0445\n",
      "Epoch [66/75], Step [200], Loss: 0.0393\n",
      "Epoch [66/75], Step [300], Loss: 0.0396\n",
      "Epoch [67/75], Step [100], Loss: 0.0367\n",
      "Epoch [67/75], Step [200], Loss: 0.0364\n",
      "Epoch [67/75], Step [300], Loss: 0.0442\n",
      "Epoch [68/75], Step [100], Loss: 0.0352\n",
      "Epoch [68/75], Step [200], Loss: 0.0349\n",
      "Epoch [68/75], Step [300], Loss: 0.0376\n",
      "Epoch [69/75], Step [100], Loss: 0.0316\n",
      "Epoch [69/75], Step [200], Loss: 0.0315\n",
      "Epoch [69/75], Step [300], Loss: 0.0352\n",
      "Epoch [70/75], Step [100], Loss: 0.0276\n",
      "Epoch [70/75], Step [200], Loss: 0.0288\n",
      "Epoch [70/75], Step [300], Loss: 0.0314\n",
      "Epoch [71/75], Step [100], Loss: 0.0283\n",
      "Epoch [71/75], Step [200], Loss: 0.0311\n",
      "Epoch [71/75], Step [300], Loss: 0.0295\n",
      "Epoch [72/75], Step [100], Loss: 0.0262\n",
      "Epoch [72/75], Step [200], Loss: 0.0277\n",
      "Epoch [72/75], Step [300], Loss: 0.0293\n",
      "Epoch [73/75], Step [100], Loss: 0.0245\n",
      "Epoch [73/75], Step [200], Loss: 0.0214\n",
      "Epoch [73/75], Step [300], Loss: 0.0234\n",
      "Epoch [74/75], Step [100], Loss: 0.0201\n",
      "Epoch [74/75], Step [200], Loss: 0.0226\n",
      "Epoch [74/75], Step [300], Loss: 0.0249\n",
      "Epoch [75/75], Step [100], Loss: 0.0216\n",
      "Epoch [75/75], Step [200], Loss: 0.0210\n",
      "Epoch [75/75], Step [300], Loss: 0.0195\n"
     ]
    }
   ],
   "source": [
    "train_model(num_epochs=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "935ce16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 93.16%\n"
     ]
    }
   ],
   "source": [
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78aafcf",
   "metadata": {},
   "source": [
    "在通道数折半且训练 epoch 数减少的情况下仍然几乎保持了准确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea80d3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "path = './models/resnet_half.pth'\n",
    "torch.save(model.state_dict(), path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
